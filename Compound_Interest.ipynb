{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Compound_Interest.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/jimmyjamesarnold/Compound_Interest/blob/master/Compound_Interest.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "68D8hnVpQbjj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "460d1e5d-b081-406b-8d4c-ef7b1122e142"
      },
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# load pydrive\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth, files\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "print('loading data...')\n",
        "# load files from drive\n",
        "train_import = drive.CreateFile({'id':'1KlafA7iNBDFrjFx4dA7RIEYF3_EqPEYi'})\n",
        "train_import.GetContentFile('train.json')\n",
        "test_import = drive.CreateFile({'id':'1SJN9ht0gaNa8OfPsOJU3PfzIFfPqKqX0'})\n",
        "test_import.GetContentFile('test.json')\n",
        "cmpd_import = drive.CreateFile({'id':'1Jqx15uTUd264d5T8zN11USUA2DGwoiLo'})\n",
        "cmpd_import.GetContentFile('labeled_standard_contents_in_foodID_by_compoundID.csv')\n",
        "\n",
        "print('loading packages...')\n",
        "# performance tracking\n",
        "from __future__ import print_function\n",
        "from pprint import pprint\n",
        "from time import time\n",
        "# Load packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# text and feature manipulation\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Models\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# SVM Classifier\n",
        "from sklearn.svm import SVC\n",
        "# OvR Classifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "print('loading data...')\n",
        "# Load foodb content and food CSVs - from other project\n",
        "cmpds = pd.read_csv('labeled_standard_contents_in_foodID_by_compoundID.csv')\n",
        "# melt, drop 0s, and lowercase names\n",
        "cmpds = pd.melt(cmpds, id_vars=['name'], var_name='cmpd', value_name='std_content')\n",
        "cmpds = cmpds[cmpds.std_content > 0]\n",
        "cmpds.cmpd = cmpds.cmpd.str.replace(' ','_')\n",
        "cmpds.cmpd = cmpds.cmpd.str.replace(',','_')\n",
        "cmpds.name = cmpds.name.str.lower()\n",
        "# construct df of measured cmpds groupby ingredient. Ignore std_content - for now.\n",
        "ing_cmpd_df = cmpds.groupby(['name'])['cmpd'].apply(','.join).reset_index()\n",
        "# Load train and test data (in pandas)\n",
        "train = pd.read_json('train.json').set_index('id')\n",
        "test = pd.read_json('test.json').set_index('id')\n",
        "\n",
        "print('getting variables...')\n",
        "# save training labels\n",
        "y = train.cuisine.copy()\n",
        "# save indices for later\n",
        "#traindex = train.index\n",
        "#testdex = test.index\n",
        "# merge sets for processing\n",
        "df = pd.concat([train.drop(\"cuisine\", axis=1), test], axis=0)\n",
        "#df_index = df.index\n",
        "\n",
        "print('engineering ingredient features...')\n",
        "# use TfidfVectorizer to one hot encode.\n",
        "tfidf = TfidfVectorizer(tokenizer=lambda x: [i.strip() for i in x.split(',')], binary=True)\n",
        "# next, define function to tokenize items in series\n",
        "def inv_count(x):  # call series from pd\n",
        "    dummies = tfidf.fit_transform(x.apply(','.join)) \n",
        "    df = pd.DataFrame(dummies.todense(),columns=tfidf.get_feature_names())\n",
        "    return df\n",
        "# Let's take an inventory of all ingredients\n",
        "train_ingr = inv_count(train.ingredients)\n",
        "#test_ingr = inv_count(test.ingredients)\n",
        "\n",
        "print('extracting compound features...')\n",
        "# get list of ingredients\n",
        "all_ingr = inv_count(df.ingredients)\n",
        "pantry = all_ingr.columns.get_values()\n",
        "# build dict to relate ingredients to compounds\n",
        "cmpd_dict = {} \n",
        "for i in pantry:\n",
        "    temp = i.replace('(','') \n",
        "    temp = temp.replace(')','')\n",
        "    temp = temp.split()\n",
        "    cmpd_list = []\n",
        "    cmpd_list.extend([','.join(ing_cmpd_df[ing_cmpd_df.name.str.contains(j)].cmpd) for j in temp])\n",
        "    cmpd_dict[i] = cmpd_list\n",
        "# map recipes to cmpds using cmpd_dict\n",
        "def rec_cmpdr(df): \n",
        "    recipe_dict = defaultdict(list) # dict for mapping recipe ingredients to compounds, uses defaultdict to build from empty list\n",
        "    for i, row in df.iterrows(): \n",
        "        for k in row.ingredients:\n",
        "            if k in cmpd_dict: # check for ingredient, if not, leave empty\n",
        "                recipe_dict[i].extend(cmpd_dict[k]) # extends list of compounds for all ingredients in recipe, stored to index\n",
        "            else: recipe_dict[i]=[] \n",
        "    return recipe_dict\n",
        "# add compounds to df\n",
        "print('engineering compound features...')\n",
        "train[\"compounds\"] = pd.Series(rec_cmpdr(train))\n",
        "#test[\"compounds\"] = pd.Series(rec_cmpdr(test)) \n",
        "# Now let's take an inventory of all compounds\n",
        "train_cmpd = inv_count(train.compounds) \n",
        "#test_cmpd = inv_count(test.compounds) \n",
        "\n",
        "# clean up memory space\n",
        "del pantry\n",
        "del all_ingr\n",
        "del df\n",
        "del cmpds\n",
        "del train"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data...\n",
            "loading packages...\n",
            "loading data...\n",
            "getting variables...\n",
            "engineering ingredient features...\n",
            "extracting compound features...\n",
            "engineering compound features...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CtzhFCA8q8Z1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "193d0eaa-56e4-4840-ee17-ab678942b51e"
      },
      "cell_type": "code",
      "source": [
        "print('joining feature sets...')\n",
        "# merge ingredient and compound data and use for training.\n",
        "print('join train and test cmpd and ingr sets...')\n",
        "# split data back into train and test\n",
        "ComboX_TRAIN = train_ingr.join(train_cmpd,lsuffix='_train_ingr', rsuffix='_train_cmpd')\n",
        "# save test case for trained model\n",
        "#ComboX_final_test = test_ingr.join(test_cmpd,lsuffix='_test_ingr', rsuffix='_test_cmpd')\n",
        "\n",
        "# clean up memory space\n",
        "del train_cmpd\n",
        "del train_ingr"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "joining feature sets...\n",
            "join train and test cmpd and ingr sets...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "daALiaCsq_YD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2117f06d-7e06-4bc4-e20b-1272371caa6b"
      },
      "cell_type": "code",
      "source": [
        "print('splitting data...')\n",
        "# split training data for early training\n",
        "X_train, X_test, y_train, y_test = train_test_split(ComboX_TRAIN,y,test_size=.3)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "splitting data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6uQw6hBPtg9l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1659
        },
        "outputId": "ba1ae0b8-2ad6-48e7-e1db-3912089d8ecf"
      },
      "cell_type": "code",
      "source": [
        "print('preparing model...')\n",
        "# Establish cross-validated GridSearchCV\n",
        "# Tunable variables:\n",
        "#C=100, # penalty parameter, setting it to a larger value \n",
        "#degree=3, # default value, not tuned yet\n",
        "#gamma=1, # kernel coefficient, not tuned yet\n",
        "# kernel type, rbf working fine here\n",
        "model_to_set = OneVsRestClassifier(SVC(\n",
        "    kernel='rbf', \n",
        "\t \tcoef0=1, # change to 1 from default value of 0.0\n",
        "\t \tshrinking=True, # using shrinking heuristics\n",
        "\t \ttol=0.001, # stopping criterion tolerance \n",
        "\t  probability=False, # no need to enable probability estimates\n",
        "\t  class_weight=None, # all classes are treated equally \n",
        "\t  verbose=False, # print the logs \n",
        "\t  max_iter=-1, # no limit, let it run\n",
        "    decision_function_shape=None, # will use one vs rest explicitly \n",
        "    random_state=42))\n",
        "\n",
        "parameters = {\n",
        "    \"estimator__C\": [1, 10],\n",
        "    \"estimator__degree\":[1, 3],\n",
        "    \"estimator__gamma\":[0.1, 1]\n",
        "}\n",
        "#\"estimator__kernel\": [\"linear\",\"rbf\",\"poly\"],\n",
        "\n",
        "print('tuning model...')\n",
        "if __name__ == \"__main__\":\n",
        "    # multiprocessing requires the fork to happen in a __main__ protected\n",
        "    # block\n",
        "\n",
        "    # find the best parameters for both the feature extraction and the\n",
        "    # classifier\n",
        "    grid_search = GridSearchCV(model_to_set, param_grid=parameters, n_jobs=-1)\n",
        "    \n",
        "    print(\"Performing grid search...\")\n",
        "    print(\"parameters:\")\n",
        "    pprint(parameters)\n",
        "    t0 = time()\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    print(\"done in %0.3fs\" % (time() - t0))\n",
        "    print()\n",
        "\n",
        "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
        "    print(\"Best parameters set:\")\n",
        "    best_parameters = grid_search.best_estimator_.get_params()\n",
        "    for param_name in sorted(parameters.keys()):\n",
        "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preparing model...\n",
            "tuning model...\n",
            "Performing grid search...\n",
            "parameters:\n",
            "{'estimator__C': [1, 10],\n",
            " 'estimator__degree': [1, 3],\n",
            " 'estimator__gamma': [0.1, 1]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-6c328aa7b7f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done in %0.3fs\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    638\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    639\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 640\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_handle_tasks\u001b[0;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[1;32m    422\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m                         \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m                         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/pool.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                 \u001b[0mCustomizablePickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reducers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/pool.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    238\u001b[0m                     print(\"Memmaping (shape=%r, dtype=%s) to new file %s\" % (\n\u001b[1;32m    239\u001b[0m                         a.shape, a.dtype, filename))\n\u001b[0;32m--> 240\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mdumped_filename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdumped_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFILE_PERMISSIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_filename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mNumpyPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mNumpyPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_framing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTOP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_framing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;31m# And then array bytes are written right after the wrapper.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mwrite_array\u001b[0;34m(self, array, pickler)\u001b[0m\n\u001b[1;32m     91\u001b[0m                                            \u001b[0mbuffersize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffersize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                                            order=self.order):\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_handle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "DkcTsaM-fnTe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Predictions \n",
        "print (\"Predict on test data ... \")\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "OvR_conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "OvR_acc_score = accuracy_score(y_test, y_pred)\n",
        "print('Tuned OvR:')\n",
        "print(OvR_conf_matrix)\n",
        "print(OvR_acc_score*100)\n",
        "\n",
        "# Submission\n",
        "print (\"Generate Submission File ... \")\n",
        "test_id = [doc['id'] for doc in test]\n",
        "sub = pd.DataFrame({'id': test_id, 'cuisine': y_pred}, columns=['id', 'cuisine'])\n",
        "sub.to_csv('svm_output.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wAr6TjdTQufK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Decision Trees\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# NaiveBayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "# RandomForest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression(multi_class= 'ovr')\n",
        "score = cross_validate(lr, X, y, return_train_score=False)\n",
        "score[\"test_score\"].mean()\n",
        "lr.fit(X_TRAIN, y_train)\n",
        "lr_predict = lr.predict(X_test)\n",
        "# print confusion matrix and accuracy score\n",
        "lr_conf_matrix = confusion_matrix(y_test, lr_predict)\n",
        "lr_acc_score = accuracy_score(y_test, lr_predict)\n",
        "print('Logistic Regression:')\n",
        "print(lr_conf_matrix)\n",
        "print(lr_acc_score*100)\n",
        "\n",
        "# Decision Trees\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_TRAIN,y_train)\n",
        "dt_predict = dt.predict(X_test)\n",
        "#print confusion matrix and accuracy score\n",
        "dt_conf_matrix = confusion_matrix(y_test, dt_predict)\n",
        "dt_acc_score = accuracy_score(y_test, dt_predict)\n",
        "print('Decision Trees:')\n",
        "print(dt_conf_matrix)\n",
        "print(dt_acc_score*100)\n",
        "\n",
        "# NaiveBayes\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_TRAIN,y_train)\n",
        "nb_predict=nb.predict(X_test)\n",
        "#print confusion matrix and accuracy score\n",
        "nb_conf_matrix = confusion_matrix(y_test, nb_predict)\n",
        "nb_acc_score = accuracy_score(y_test, nb_predict)\n",
        "print('NaiveBayes:')\n",
        "print(nb_conf_matrix)\n",
        "print(nb_acc_score*100)\n",
        "\n",
        "# RandomForest\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_TRAIN, y_train)\n",
        "rf_predict=rf.predict(X_test)\n",
        "#print confusion matrix and accuracy score\n",
        "rf_conf_matrix = confusion_matrix(y_test, rf_predict)\n",
        "rf_acc_score = accuracy_score(y_test, rf_predict)\n",
        "print('RandomForest:')\n",
        "print(rf_conf_matrix)\n",
        "print(rf_acc_score*100)\n",
        "\n",
        "#first test the linear kernel first and check the accuracy\n",
        "lin_svc = SVC(kernel='linear')\n",
        "lin_svc.fit(X_TRAIN, y_train)\n",
        "lin_svc=lin_svc.predict(X_test)\n",
        "#print confusion matrix and accuracy score\n",
        "lin_svc_conf_matrix = confusion_matrix(y_test, rf_predict)\n",
        "lin_svc_acc_score = accuracy_score(y_test, rf_predict)\n",
        "print('Linear SVC:')\n",
        "print(lin_svc_conf_matrix)\n",
        "print(lin_svc_acc_score*100)\n",
        "\n",
        "#second try the rbf kernel\n",
        "rbf_svc = SVC(kernel='rbf')\n",
        "rbf_svc.fit(X_TRAIN, y_train)\n",
        "rbf_svc=rbf_svc.predict(X_test)\n",
        "rbf_svc_conf_matrix = confusion_matrix(y_test, rf_predict)\n",
        "rbf_svc_acc_score = accuracy_score(y_test, rf_predict)\n",
        "print('rbf SVC:')\n",
        "print(rbf_svc_conf_matrix)\n",
        "print(rbf_svc_acc_score*100)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}