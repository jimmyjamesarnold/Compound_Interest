{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Compound_Interest.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/jimmyjamesarnold/Compound_Interest/blob/master/Compound_Interest.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "8GfQML8fRLsA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a6b1785-0701-41a5-d5f5-59cee19bbe34"
      },
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# load pydrive\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth, files\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-08-24 05:01:50,230 WARNING No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "68D8hnVpQbjj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "289509ad-5024-492b-8256-49b0225af562"
      },
      "cell_type": "code",
      "source": [
        "# performance tracking\n",
        "from __future__ import print_function\n",
        "from pprint import pprint\n",
        "from time import time\n",
        "import logging\n",
        "\n",
        "# Load packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# text and feature manipulation\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Models\n",
        "from sklearn.model_selection import cross_validate, train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "# logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Decision Trees\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# NaiveBayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "# RandomForest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# SVM Classifier\n",
        "from sklearn.svm import SVC\n",
        "# OvR Classifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "print(__doc__)\n",
        "# Display progress logs on stdout\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s %(levelname)s %(message)s')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Automatically created module for IPython interactive environment\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UbjT0IviR1Hh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "outputId": "2739453c-c2fa-4113-9b4c-86771cc25962"
      },
      "cell_type": "code",
      "source": [
        "# load files from drive\n",
        "train_import = drive.CreateFile({'id':'1KlafA7iNBDFrjFx4dA7RIEYF3_EqPEYi'})\n",
        "train_import.GetContentFile('train.json')\n",
        "\n",
        "test_import = drive.CreateFile({'id':'1SJN9ht0gaNa8OfPsOJU3PfzIFfPqKqX0'})\n",
        "test_import.GetContentFile('test.json')\n",
        "\n",
        "cmpd_import = drive.CreateFile({'id':'1Jqx15uTUd264d5T8zN11USUA2DGwoiLo'})\n",
        "cmpd_import.GetContentFile('labeled_standard_contents_in_foodID_by_compoundID.csv')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-08-24 05:02:13,692 WARNING file_cache is unavailable when using oauth2client >= 4.0.0\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
            "    from google.appengine.api import memcache\n",
            "ModuleNotFoundError: No module named 'google.appengine'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
            "    from oauth2client.contrib.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
            "    from oauth2client.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
            "    from . import file_cache\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
            "    'file_cache is unavailable when using oauth2client >= 4.0.0')\n",
            "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0\n",
            "2018-08-24 05:02:13,699 INFO URL being requested: GET https://www.googleapis.com/discovery/v1/apis/drive/v2/rest\n",
            "2018-08-24 05:02:13,702 INFO Attempting refresh to obtain initial access_token\n",
            "2018-08-24 05:02:13,704 INFO Refreshing access_token\n",
            "2018-08-24 05:02:13,815 INFO URL being requested: GET https://www.googleapis.com/drive/v2/files/1KlafA7iNBDFrjFx4dA7RIEYF3_EqPEYi?alt=json\n",
            "2018-08-24 05:02:14,495 INFO URL being requested: GET https://www.googleapis.com/drive/v2/files/1SJN9ht0gaNa8OfPsOJU3PfzIFfPqKqX0?alt=json\n",
            "2018-08-24 05:02:15,031 INFO URL being requested: GET https://www.googleapis.com/drive/v2/files/1Jqx15uTUd264d5T8zN11USUA2DGwoiLo?alt=json\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "LNx34GN9Sxlk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "outputId": "f650f6a9-c98b-4ccb-cbc5-cffdfb96e45a"
      },
      "cell_type": "code",
      "source": [
        "# Load foodb content and food CSVs - from other project\n",
        "cmpds = pd.read_csv('labeled_standard_contents_in_foodID_by_compoundID.csv')\n",
        "# melt, drop 0s, and lowercase names\n",
        "cmpds = pd.melt(cmpds, id_vars=['name'], var_name='cmpd', value_name='std_content')\n",
        "cmpds = cmpds[cmpds.std_content > 0]\n",
        "cmpds.cmpd = cmpds.cmpd.str.replace(' ','_')\n",
        "cmpds.cmpd = cmpds.cmpd.str.replace(',','_')\n",
        "cmpds.name = cmpds.name.str.lower()\n",
        "# construct df of measured cmpds groupby ingredient. Ignore std_content - for now.\n",
        "ing_cmpd_df = cmpds.groupby(['name'])['cmpd'].apply(','.join).reset_index()\n",
        "\n",
        "# Load train and test data (in pandas)\n",
        "train = pd.read_json('train.json').set_index('id')\n",
        "test = pd.read_json('test.json').set_index('id')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2730475daec5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcmpds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'labeled_standard_contents_in_foodID_by_compoundID.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# melt, drop 0s, and lowercase names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcmpds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmpds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cmpd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'std_content'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcmpds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmpds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcmpds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd_content\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcmpds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmpd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmpds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: File b'labeled_standard_contents_in_foodID_by_compoundID.csv' does not exist"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "grDB1FoBQlGM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "outputId": "6a17cdd5-87f6-4ce5-b559-d2a4c5aca6cd"
      },
      "cell_type": "code",
      "source": [
        "## Load train and test data (in json)\n",
        "#import json\n",
        "## Dataset Preparation\n",
        "#print (\"Read Dataset ... \")\n",
        "#def read_dataset(path):\n",
        "#\treturn json.load(open(path))\n",
        "#train = read_dataset('train.json')\n",
        "#test = read_dataset('test.json')\n",
        "\n",
        "# save indices for later\n",
        "traindex = train.index\n",
        "testdex = test.index\n",
        "# merge sets for processing\n",
        "df = pd.concat([train.drop(\"cuisine\", axis=1), test], axis=0)\n",
        "df_index = df.index\n",
        "\n",
        "# Text Data Features\n",
        "print (\"Prepare text data of Train and Test ... \")\n",
        "def generate_text(data):\n",
        "\ttext_data = [\" \".join(doc['ingredients']).lower() for doc in data]\n",
        "\treturn text_data \n",
        "\n",
        "train_text = generate_text(train)\n",
        "test_text = generate_text(test)\n",
        "target = [doc['cuisine'] for doc in train]\n",
        "\n",
        "## Label Encoding - Target \n",
        "#print (\"Label Encode the Target Variable ... \")\n",
        "#lb = LabelEncoder()\n",
        "#y = lb.fit_transform(target)\n",
        "\n",
        "# simple EDA\n",
        "sns.countplot(y=y, order=y.value_counts().reset_index()[\"index\"])\n",
        "plt.title(\"Cuisine Distribution\")\n",
        "plt.show()\n",
        "\n",
        "# feature engineering\n",
        "print (\"TF-IDF on text data ... \")\n",
        "def tfidf_features(txt, flag):\n",
        "    if flag == \"train\":\n",
        "    \tx = tfidf.fit_transform(txt)\n",
        "    else:\n",
        "\t    x = tfidf.transform(txt)\n",
        "    x = x.astype('float16')\n",
        "    return x \n",
        "\n",
        "X = tfidf_features(train_text, flag=\"train\")\n",
        "X_test = tfidf_features(test_text, flag=\"test\")\n",
        "\n",
        "## first, use Sklearn's CountVectorizer to one hot encode.\n",
        "#vect = CountVectorizer(tokenizer=lambda x: [i.strip() for i in x.split(',')])\n",
        "\n",
        "# or, use Sklearn's TfidfVectorizer to one hot encode.\n",
        "tfidf = TfidfVectorizer(tokenizer=lambda x: [i.strip() for i in x.split(',')], binary=True)\n",
        "\n",
        "# next, define function to tokenize items in series\n",
        "def inv_count(x):  # call series from pd\n",
        "    dummies = tfidf.fit_transform(x.apply(','.join)) \n",
        "    df = pd.DataFrame(dummies.todense(),columns=tfidf.get_feature_names())\n",
        "    return df\n",
        "# Let's take an inventory of all ingredients\n",
        "all_ingr = inv_count(df.ingredients)    \n",
        "\n",
        "# get list of ingredients\n",
        "pantry = all_ingr.columns.get_values()\n",
        "# build dict to relate ingredients to compounds\n",
        "cmpd_dict = {} \n",
        "for i in pantry:\n",
        "    temp = i.replace('(','') \n",
        "    temp = temp.replace(')','')\n",
        "    temp = temp.split()\n",
        "    cmpd_list = []\n",
        "    cmpd_list.extend([','.join(ing_cmpd_df[ing_cmpd_df.name.str.contains(j)].cmpd) for j in temp])\n",
        "    cmpd_dict[i] = cmpd_list\n",
        "\n",
        "# map recipes to cmpds using cmpd_dict\n",
        "def rec_cmpdr(df): \n",
        "    recipe_dict = defaultdict(list) # dict for mapping recipe ingredients to compounds, uses defaultdict to build from empty list\n",
        "    for i, row in df.iterrows(): \n",
        "        for k in row.ingredients:\n",
        "            if k in cmpd_dict: # check for ingredient, if not, leave empty\n",
        "                recipe_dict[i].extend(cmpd_dict[k]) # extends list of compounds for all ingredients in recipe, stored to index\n",
        "            else: recipe_dict[i]=[] \n",
        "    return recipe_dict\n",
        "# add compounds to df\n",
        "df[\"compounds\"] = pd.Series(rec_cmpdr(df)) \n",
        "# Now let's take an inventory of all compounds\n",
        "all_cmpd = inv_count(df.compounds) \n",
        "\n",
        "# merge ingredient and compound data and use for training.\n",
        "X = all_ingr.join(all_cmpd,lsuffix='_all_ingr', rsuffix='_all_cmpd')\n",
        "# split data back into train and test\n",
        "X_train = X.loc[traindex,:]\n",
        "# save test case for trained model\n",
        "final_test = X.loc[testdex,:]\n",
        "# split training data for early training\n",
        "X_TRAIN, X_test, y_train, y_test = train_test_split(X_train,y,test_size=.3,random_state=42)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-490acb29ccb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcmpds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'labeled_standard_contents_in_foodID_by_compoundID.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# melt, drop 0s, and lowercase names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcmpds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmpds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cmpd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'std_content'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcmpds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmpds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcmpds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd_content\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcmpds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmpd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmpds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: File b'labeled_standard_contents_in_foodID_by_compoundID.csv' does not exist"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "wAr6TjdTQufK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression(multi_class= 'ovr')\n",
        "score = cross_validate(lr, X, y, return_train_score=False)\n",
        "score[\"test_score\"].mean()\n",
        "lr.fit(X_TRAIN, y_train)\n",
        "lr_predict = lr.predict(X_test)\n",
        "# print confusion matrix and accuracy score\n",
        "lr_conf_matrix = confusion_matrix(y_test, lr_predict)\n",
        "lr_acc_score = accuracy_score(y_test, lr_predict)\n",
        "print('Logistic Regression:')\n",
        "print(lr_conf_matrix)\n",
        "print(lr_acc_score*100)\n",
        "\n",
        "# Decision Trees\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_TRAIN,y_train)\n",
        "dt_predict = dt.predict(X_test)\n",
        "#print confusion matrix and accuracy score\n",
        "dt_conf_matrix = confusion_matrix(y_test, dt_predict)\n",
        "dt_acc_score = accuracy_score(y_test, dt_predict)\n",
        "print('Decision Trees:')\n",
        "print(dt_conf_matrix)\n",
        "print(dt_acc_score*100)\n",
        "\n",
        "# NaiveBayes\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_TRAIN,y_train)\n",
        "nb_predict=nb.predict(X_test)\n",
        "#print confusion matrix and accuracy score\n",
        "nb_conf_matrix = confusion_matrix(y_test, nb_predict)\n",
        "nb_acc_score = accuracy_score(y_test, nb_predict)\n",
        "print('NaiveBayes:')\n",
        "print(nb_conf_matrix)\n",
        "print(nb_acc_score*100)\n",
        "\n",
        "# RandomForest\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_TRAIN, y_train)\n",
        "rf_predict=rf.predict(X_test)\n",
        "#print confusion matrix and accuracy score\n",
        "rf_conf_matrix = confusion_matrix(y_test, rf_predict)\n",
        "rf_acc_score = accuracy_score(y_test, rf_predict)\n",
        "print('RandomForest:')\n",
        "print(rf_conf_matrix)\n",
        "print(rf_acc_score*100)\n",
        "\n",
        "#first test the linear kernel first and check the accuracy\n",
        "lin_svc = SVC(kernel='linear')\n",
        "lin_svc.fit(X_TRAIN, y_train)\n",
        "lin_svc=lin_svc.predict(X_test)\n",
        "#print confusion matrix and accuracy score\n",
        "lin_svc_conf_matrix = confusion_matrix(y_test, rf_predict)\n",
        "lin_svc_acc_score = accuracy_score(y_test, rf_predict)\n",
        "print('Linear SVC:')\n",
        "print(lin_svc_conf_matrix)\n",
        "print(lin_svc_acc_score*100)\n",
        "\n",
        "#second try the rbf kernel\n",
        "rbf_svc = SVC(kernel='rbf')\n",
        "rbf_svc.fit(X_TRAIN, y_train)\n",
        "rbf_svc=rbf_svc.predict(X_test)\n",
        "rbf_svc_conf_matrix = confusion_matrix(y_test, rf_predict)\n",
        "rbf_svc_acc_score = accuracy_score(y_test, rf_predict)\n",
        "print('rbf SVC:')\n",
        "print(rbf_svc_conf_matrix)\n",
        "print(rbf_svc_acc_score*100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ineq13tdQzpq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Establish cross-validated GridSearchCV\n",
        "\n",
        "# Model Training \n",
        "print (\"Train the model ... \")\n",
        "classifier = SVC(C=100, # penalty parameter, setting it to a larger value \n",
        "\t \t\t\t kernel='rbf', # kernel type, rbf working fine here\n",
        "\t \t\t\t degree=3, # default value, not tuned yet\n",
        "\t \t\t\t gamma=1, # kernel coefficient, not tuned yet\n",
        "\t \t\t\t coef0=1, # change to 1 from default value of 0.0\n",
        "\t \t\t\t shrinking=True, # using shrinking heuristics\n",
        "\t \t\t\t tol=0.001, # stopping criterion tolerance \n",
        "\t      \t\t probability=False, # no need to enable probability estimates\n",
        "\t      \t\t cache_size=200, # 200 MB cache size\n",
        "\t      \t\t class_weight=None, # all classes are treated equally \n",
        "\t      \t\t verbose=False, # print the logs \n",
        "\t      \t\t max_iter=-1, # no limit, let it run\n",
        "          \t\t decision_function_shape=None, # will use one vs rest explicitly \n",
        "          \t\t random_state=None)\n",
        "model = OneVsRestClassifier(classifier, n_jobs=4)\n",
        "model.fit(X, y)\n",
        "\n",
        "\n",
        "# Predictions \n",
        "print (\"Predict on test data ... \")\n",
        "y_test = model.predict(X_test)\n",
        "y_pred = lb.inverse_transform(y_test)\n",
        "\n",
        "\n",
        "# Submission\n",
        "print (\"Generate Submission File ... \")\n",
        "test_id = [doc['id'] for doc in test]\n",
        "sub = pd.DataFrame({'id': test_id, 'cuisine': y_pred}, columns=['id', 'cuisine'])\n",
        "sub.to_csv('svm_output.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}