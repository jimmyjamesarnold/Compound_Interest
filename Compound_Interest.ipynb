{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Compound_Interest.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/jimmyjamesarnold/Compound_Interest/blob/master/Compound_Interest.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "68D8hnVpQbjj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5f3c1a03-e52a-4634-c5d5-520b4eca9565"
      },
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# load pydrive\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth, files\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "print('loading data...')\n",
        "# load files from drive\n",
        "train_import = drive.CreateFile({'id':'1KlafA7iNBDFrjFx4dA7RIEYF3_EqPEYi'})\n",
        "train_import.GetContentFile('train.json')\n",
        "test_import = drive.CreateFile({'id':'1SJN9ht0gaNa8OfPsOJU3PfzIFfPqKqX0'})\n",
        "test_import.GetContentFile('test.json')\n",
        "\n",
        "print('loading packages...')\n",
        "# performance tracking\n",
        "from __future__ import print_function\n",
        "from pprint import pprint\n",
        "from time import time\n",
        "# Load packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# text and feature manipulation\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Models\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score\n",
        "# SVM Classifier\n",
        "from sklearn.svm import SVC\n",
        "# OvR Classifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "print('loading data...')\n",
        "# load files from drive\n",
        "# Load train and test data (in pandas)\n",
        "train = pd.read_json('train.json').set_index('id')\n",
        "test = pd.read_json('test.json').set_index('id')\n",
        "\n",
        "print('engineering ingredient features...')\n",
        "# save training labels\n",
        "y = train.cuisine.copy()\n",
        "\n",
        "# use TfidfVectorizer to one hot encode.\n",
        "tfidf = TfidfVectorizer(tokenizer=lambda x: [i.strip() for i in x.split(',')])\n",
        "\n",
        "# next, define function to tokenize items in series\n",
        "def inv_count(x):  # call series from pd\n",
        "    dummies = tfidf.fit_transform(x.apply(','.join)) \n",
        "    df = pd.DataFrame(dummies.todense(),columns=tfidf.get_feature_names())\n",
        "    return df\n",
        "\n",
        "# Vectorize inventory of all ingredients\n",
        "train_ingr = inv_count(train.ingredients)\n",
        "test_ingr = inv_count(test.ingredients)\n",
        "\n",
        "# clean up memory space\n",
        "del train_import\n",
        "del test_import"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data...\n",
            "loading packages...\n",
            "loading data...\n",
            "engineering ingredient features...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vivkqlMLIQrn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('split ingredient data...')\n",
        "# split training data for early training\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_ingr,y,test_size=.4,random_state=42,stratify=y)\n",
        "\n",
        "# Set up OvR using SVC using rbf kernel\n",
        "print('preparing model...')\n",
        "model = SVC(coef0=1, # change to 1 from default value of 0.0\n",
        "             shrinking=True, # using shrinking heuristics\n",
        "             verbose=True, # print the logs \n",
        "             max_iter=-1, # no limit, let it run\n",
        "             random_state=42)\n",
        "# parameters to tune:\n",
        "# C\n",
        "# gamma\n",
        "parameters = {\n",
        "    'C':[1,10,100,1000],\n",
        "    'gamma':[1,0.1,0.001,0.0001]\n",
        "             }\n",
        "\n",
        "print('split data for hyperparameter tuning ...')\n",
        "# split super small subset for hyperparameter tuning\n",
        "X_tune, X_tune2, y_tune, y_tune2 = train_test_split(X_train,y_train,test_size=.8,random_state=42,stratify=y_train)\n",
        "\n",
        "# Set up GridSearchCV\n",
        "print(\"Performing grid search...\")\n",
        "grid_search = GridSearchCV(model, param_grid=parameters, verbose=2)\n",
        "grid_search.fit(X_tune, y_tune)\n",
        "\n",
        "print(\"Best parameters:\")\n",
        "print(grid_search.best_score_)\n",
        "print(grid_search.best_params_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6uQw6hBPtg9l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a9-SlwNHCO3O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run classification report\n",
        "print(\"The model is trained on the full development set.\")\n",
        "print(\"The scores are computed on the full evaluation set.\")\n",
        "print()\n",
        "y_true, y_pred = y_test, clf.predict(X_test)\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NbEq-o5z9ZaH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Compound Engineering\n",
        "\n",
        "print('loading data...')\n",
        "# Load foodb content and food CSVs - from other project\n",
        "cmpd_import = drive.CreateFile({'id':'1Jqx15uTUd264d5T8zN11USUA2DGwoiLo'})\n",
        "cmpd_import.GetContentFile('labeled_standard_contents_in_foodID_by_compoundID.csv')\n",
        "cmpds = pd.read_csv('labeled_standard_contents_in_foodID_by_compoundID.csv')\n",
        "# melt, drop 0s, and lowercase names\n",
        "cmpds = pd.melt(cmpds, id_vars=['name'], var_name='cmpd', value_name='std_content')\n",
        "cmpds = cmpds[cmpds.std_content > 0]\n",
        "cmpds.cmpd = cmpds.cmpd.str.replace(' ','_')\n",
        "cmpds.cmpd = cmpds.cmpd.str.replace(',','_')\n",
        "cmpds.name = cmpds.name.str.lower()\n",
        "# construct df of measured cmpds groupby ingredient. Ignore std_content - for now.\n",
        "ing_cmpd_df = cmpds.groupby(['name'])['cmpd'].apply(','.join).reset_index()\n",
        "\n",
        "# merge train and test sets from above for compound processing\n",
        "df = pd.concat([train.drop(\"cuisine\", axis=1), test], axis=0)\n",
        "\n",
        "print('extracting compound features...')\n",
        "# get list of ingredients\n",
        "all_ingr = inv_count(df.ingredients)\n",
        "pantry = all_ingr.columns.get_values()\n",
        "# build dict to relate ingredients to compounds\n",
        "cmpd_dict = {} \n",
        "for i in pantry:\n",
        "    temp = i.replace('(','') \n",
        "    temp = temp.replace(')','')\n",
        "    temp = temp.split()\n",
        "    cmpd_list = []\n",
        "    cmpd_list.extend([','.join(ing_cmpd_df[ing_cmpd_df.name.str.contains(j)].cmpd) for j in temp])\n",
        "    cmpd_dict[i] = cmpd_list\n",
        "\n",
        "# map recipes to cmpds using cmpd_dict - there must be a more efficient way to do this\n",
        "def rec_cmpdr(df): \n",
        "    recipe_dict = defaultdict(list) # dict for mapping recipe ingredients to compounds, uses defaultdict to build from empty list\n",
        "    for i, row in df.iterrows(): \n",
        "        for k in row.ingredients:\n",
        "            if k in cmpd_dict: # check for ingredient, if not, leave empty\n",
        "                recipe_dict[i].extend(cmpd_dict[k]) # extends list of compounds for all ingredients in recipe, stored to index\n",
        "            else: recipe_dict[i]=[] \n",
        "    return recipe_dict\n",
        "  \n",
        "# add compounds to df's\n",
        "print('engineering compound features...')\n",
        "train[\"compounds\"] = pd.Series(rec_cmpdr(train))\n",
        "test[\"compounds\"] = pd.Series(rec_cmpdr(test)) \n",
        "\n",
        "# Now let's take an inventory of all compounds\n",
        "train_cmpd = inv_count(train.compounds) \n",
        "test_cmpd = inv_count(test.compounds) \n",
        "\n",
        "# clean up memory space\n",
        "del cmpd_import\n",
        "del cmpds\n",
        "del ing_cmpd_df\n",
        "del df\n",
        "del all_ingr\n",
        "del pantry\n",
        "del cmpd_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CtzhFCA8q8Z1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "193d0eaa-56e4-4840-ee17-ab678942b51e"
      },
      "cell_type": "code",
      "source": [
        "print('join train and test cmpd and ingr sets...')\n",
        "# split data back into train and test\n",
        "ComboX_TRAIN = train_ingr.join(train_cmpd,lsuffix='_train_ingr', rsuffix='_train_cmpd')\n",
        "# save test case for trained model\n",
        "ComboX_final_test = test_ingr.join(test_cmpd,lsuffix='_test_ingr', rsuffix='_test_cmpd')\n",
        "\n",
        "# clean up memory space\n",
        "del train_cmpd\n",
        "del train_ingr\n",
        "del test_cmpd\n",
        "del test_ingr"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "joining feature sets...\n",
            "join train and test cmpd and ingr sets...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DkcTsaM-fnTe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Predictions \n",
        "print (\"Predict on test data ... \")\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "OvR_conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "OvR_acc_score = accuracy_score(y_test, y_pred)\n",
        "print('Tuned OvR:')\n",
        "print(OvR_conf_matrix)\n",
        "print(OvR_acc_score*100)\n",
        "\n",
        "# Submission\n",
        "print (\"Generate Submission File ... \")\n",
        "test_id = [doc['id'] for doc in test]\n",
        "sub = pd.DataFrame({'id': test_id, 'cuisine': y_pred}, columns=['id', 'cuisine'])\n",
        "sub.to_csv('svm_output.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wAr6TjdTQufK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Other models for testing\n",
        "\n",
        "# logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Decision Trees\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# NaiveBayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "# RandomForest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression(multi_class= 'ovr')\n",
        "score = cross_validate(lr, X, y, return_train_score=False)\n",
        "score[\"test_score\"].mean()\n",
        "lr.fit(X_TRAIN, y_train)\n",
        "lr_predict = lr.predict(X_test)\n",
        "# print confusion matrix and accuracy score\n",
        "lr_conf_matrix = confusion_matrix(y_test, lr_predict)\n",
        "lr_acc_score = accuracy_score(y_test, lr_predict)\n",
        "print('Logistic Regression:')\n",
        "print(lr_conf_matrix)\n",
        "print(lr_acc_score*100)\n",
        "\n",
        "# Decision Trees\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_TRAIN,y_train)\n",
        "dt_predict = dt.predict(X_test)\n",
        "#print confusion matrix and accuracy score\n",
        "dt_conf_matrix = confusion_matrix(y_test, dt_predict)\n",
        "dt_acc_score = accuracy_score(y_test, dt_predict)\n",
        "print('Decision Trees:')\n",
        "print(dt_conf_matrix)\n",
        "print(dt_acc_score*100)\n",
        "\n",
        "# NaiveBayes\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_TRAIN,y_train)\n",
        "nb_predict=nb.predict(X_test)\n",
        "#print confusion matrix and accuracy score\n",
        "nb_conf_matrix = confusion_matrix(y_test, nb_predict)\n",
        "nb_acc_score = accuracy_score(y_test, nb_predict)\n",
        "print('NaiveBayes:')\n",
        "print(nb_conf_matrix)\n",
        "print(nb_acc_score*100)\n",
        "\n",
        "# RandomForest\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_TRAIN, y_train)\n",
        "rf_predict=rf.predict(X_test)\n",
        "#print confusion matrix and accuracy score\n",
        "rf_conf_matrix = confusion_matrix(y_test, rf_predict)\n",
        "rf_acc_score = accuracy_score(y_test, rf_predict)\n",
        "print('RandomForest:')\n",
        "print(rf_conf_matrix)\n",
        "print(rf_acc_score*100)\n",
        "\n",
        "#first test the linear kernel first and check the accuracy\n",
        "lin_svc = SVC(kernel='linear')\n",
        "lin_svc.fit(X_TRAIN, y_train)\n",
        "lin_svc=lin_svc.predict(X_test)\n",
        "#print confusion matrix and accuracy score\n",
        "lin_svc_conf_matrix = confusion_matrix(y_test, rf_predict)\n",
        "lin_svc_acc_score = accuracy_score(y_test, rf_predict)\n",
        "print('Linear SVC:')\n",
        "print(lin_svc_conf_matrix)\n",
        "print(lin_svc_acc_score*100)\n",
        "\n",
        "#second try the rbf kernel\n",
        "rbf_svc = SVC(kernel='rbf')\n",
        "rbf_svc.fit(X_TRAIN, y_train)\n",
        "rbf_svc=rbf_svc.predict(X_test)\n",
        "rbf_svc_conf_matrix = confusion_matrix(y_test, rf_predict)\n",
        "rbf_svc_acc_score = accuracy_score(y_test, rf_predict)\n",
        "print('rbf SVC:')\n",
        "print(rbf_svc_conf_matrix)\n",
        "print(rbf_svc_acc_score*100)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}